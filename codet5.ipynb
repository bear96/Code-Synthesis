{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3KHBw7g5STC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torchtext.data import utils\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from StaQCDataset import get_dataloader, get_datasets\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Constants\n",
    "BATCH_SIZE = 8\n",
    "EPOCH_SIZE = 5\n",
    "LEARNING_RATE = .0001\n",
    "NL_SEQ_LEN = 20\n",
    "CODE_SEQ_LEN = 150\n",
    "\n",
    "RESUME_EPOCH = 0                            # Epoch to resume at (0 to start from the beginning)\n",
    "\n",
    "LOG_STEP = 1                                # Frequency of epoch's for logging\n",
    "PARAMS_FILE = './training_parameters.json'  # File where input parameters (from cmdline) are stored\n",
    "PL = 'python'                               # Programming language for fine-tuning the pretrained model\n",
    "DATA_SIZE = 2000                            # Size of the raw dataset that will be used (batch size * 4 is just for testing)\n",
    "MODEL_PATH = './checkpoints/codet5_' + PL   # path to store the trained model\n",
    "MODEL_TYPE = 'codet5'\n",
    "LOG_FILE = './logs/log_' + MODEL_TYPE + '_' + PL + '.txt'   # Base file path for storing model training \n",
    "MODE = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For rlogin training (COMMENT OUT WHEN RUNNING IPYNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Enviornment Variables from extenal file \n",
    "if os.path.exists(PARAMS_FILE):\n",
    "    # params = pd.read_csv(PARAMS_FILE)\n",
    "    with open(PARAMS_FILE) as f:\n",
    "        params = json.load(f)\n",
    "    BATCH_SIZE = params['batch_size']\n",
    "    EPOCH_SIZE = params['epoch_size']\n",
    "    LEARNING_RATE = params['learning_rate']\n",
    "    NL_SEQ_LEN = params['nl_seq_len']\n",
    "    CODE_SEQ_LEN = params['code_seq_len']\n",
    "    RESUME_EPOCH = params['resume_epoch']\n",
    "    PL = params['pl_task']\n",
    "    DATA_SIZE = params['data_size']\n",
    "    MODEL_TYPE = params['pretrained']\n",
    "    LOG_STEP = params['log_step']\n",
    "    LOG_FILE = './logs/log_' + params['pretrained'] + '_' + params['pl_task'] + '.txt'\n",
    "    MODE = params['mode']\n",
    "    \n",
    "    # Check if files exist       \n",
    "    MODEL_PATH = './checkpoints/' + params['pretrained'] + '_' + params['pl_task']\n",
    "    if not os.path.isdir(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUNNING: {} with pl={}, mode={}, batch_size={}, epoch_size={}, learning_rate={}, nl_seq_len={}, code_seq_len={}, data_size={}, log_step={}, log_file={}, model_dir={} resume_epoch={}\".format(MODEL_TYPE, PL, MODE, BATCH_SIZE, EPOCH_SIZE, LEARNING_RATE, NL_SEQ_LEN, CODE_SEQ_LEN, DATA_SIZE, LOG_STEP, LOG_FILE, MODEL_PATH, RESUME_EPOCH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz16RFfhGfy7"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data, tokenizer=None, seq_len=(20, 150)):\n",
    "    # Dissect data\n",
    "    code, nl = list(data['code']), list(data['nl'])\n",
    "    nl_len, code_len = seq_len\n",
    "    \n",
    "    # source\n",
    "    if tokenizer == None:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    \n",
    "#     code = data['code']\n",
    "#     code_tokens=tokenizer.tokenize(code)[:150-2]\n",
    "#     source_tokens =[tokenizer.cls_token]+code_tokens+[tokenizer.sep_token]\n",
    "#     source_ids =  tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "#     padding_length = code_len - len(source_ids)\n",
    "#     source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "#     summary = data['nl']\n",
    "#     summary_tokens=tokenizer.tokenize(summary)[:20-2]\n",
    "#     target_tokens =[tokenizer.cls_token]+summary_tokens+[tokenizer.sep_token]\n",
    "#     target_ids =  tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "#     padding_length = nl_len - len(target_ids)\n",
    "#     target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "    \n",
    "    # # Create attention masks for input_ids\n",
    "    # source_mask = source_ids.ne(tokenizer.pad_token_id)\n",
    "    # target_mask = target_ids.ne(tokenizer.pad_token_id)\n",
    "    \n",
    "    inputs = tokenizer(nl, return_tensors='pt', max_length=nl_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(code, return_tensors=\"pt\", max_length=code_len, padding=\"max_length\", truncation=True)\n",
    "    # source_tokens = None\n",
    "    source_ids = inputs.input_ids\n",
    "    source_mask = inputs.attention_mask\n",
    "    # target_tokens = None\n",
    "    target_ids = labels.input_ids\n",
    "    target_mask = labels.attention_mask\n",
    "\n",
    "    InputFeatures = namedtuple(\"InputFeatures\", \n",
    "                               \"input_ids input_mask target_ids target_mask\")\n",
    "    \n",
    "    return InputFeatures(source_ids, source_mask, \n",
    "                         target_ids, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataLoader\n",
    "data_path = './staqc_data/' + PL\n",
    "train_loader, test_loader, val_loader = get_dataloader(data_path, convert_examples_to_features, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load trained and saved model if needed\n",
    "model = model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')\n",
    "if RESUME_EPOCH > 0:\n",
    "    saved_model_path = '{}/Epoch{}.pkl'.format(MODEL_PATH, str(RESUME_EPOCH))\n",
    "    if os.path.exists(saved_model_path):\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "    else:\n",
    "        print(\"WARNING: {} saved model does not exist! Training {} model from the epoch 0.\".format(saved_model_path, MODEL_TYPE))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BqGQtJ9KKQhZ",
    "outputId": "8a3fb2aa-3ece-496b-fbb4-b5aa2566cc5f"
   },
   "outputs": [],
   "source": [
    "if MODE == 'train':  \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Create a training scheduler for learning rate stepping\n",
    "    num_training_steps = EPOCH_SIZE * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(name='linear', optimizer=optimizer, num_training_steps=num_training_steps, num_warmup_steps=0)\n",
    "\n",
    "    # Log training\n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "\n",
    "    # open log file\n",
    "    if RESUME_EPOCH > 0:\n",
    "        log_f = open(LOG_FILE, 'a')\n",
    "    else:\n",
    "        log_f = open(LOG_FILE, 'w')\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(RESUME_EPOCH, EPOCH_SIZE):\n",
    "\n",
    "        # Initialize batch loss vars\n",
    "        model.train()\n",
    "        running_loss = list()\n",
    "        for batch in train_loader:\n",
    "\n",
    "            # Dissect batch\n",
    "            input_ids, input_mask, target_ids, target_mask = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            target_mask = target_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids = input_ids, attention_mask=input_mask, labels=target_ids, decoder_attention_mask=target_mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Gradients\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        curr_train_loss = np.array(running_loss).mean()\n",
    "        train_loss.append(curr_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss = list()\n",
    "        for batch in val_loader:\n",
    "            # Dissect batch\n",
    "            input_ids, input_mask, target_ids, target_mask = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            target_mask = target_mask.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids = input_ids, attention_mask=input_mask, labels=target_ids, decoder_attention_mask=target_mask)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        curr_val_loss = np.array(running_loss).mean()\n",
    "        val_loss.append(curr_val_loss)\n",
    "\n",
    "        # Log loss for epoch\n",
    "        if epoch % LOG_STEP == 0:\n",
    "            loss_log = \"Epoch: [{}/{}], Training Loss: {:02.6f}, Validation Loss: {:02.6f}\".format(epoch + 1, EPOCH_SIZE, curr_train_loss, curr_val_loss)\n",
    "            log_f.write(loss_log + '\\n')\n",
    "            print(loss_log)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), '{}/Epoch{}.pkl'.format(MODEL_PATH, str(epoch+1)))\n",
    "\n",
    "        # Quit if the the validation loss passes training loss for more than 2 epochs\n",
    "        val_pass = True\n",
    "        for t, v in zip(train_loss[-2:], val_loss[-2:]):\n",
    "            val_pass = val_pass and (v > t)\n",
    "\n",
    "        if val_pass and len(train_loss) > 3:\n",
    "            print(\"EARLY STOP: Validation loss was greater than training loss for the last 3 epochs!\")\n",
    "            break\n",
    "\n",
    "    log_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph Training vs Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'train':\n",
    "    train_loss, val_loss = list(), list()\n",
    "    for line in open(LOG_FILE):\n",
    "        split_line = line.split(' ')\n",
    "        train_loss.append(float(split_line[4].strip().replace(',', '')))  # Pull training loss from logs (from rlogin)\n",
    "        val_loss.append(float(split_line[7].strip()))    # pull validation loss from logs\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(MODEL_TYPE.upper() + \" Loss vs Epoch\")\n",
    "    plt.plot(train_loss,'k')\n",
    "    plt.plot(val_loss,'y')\n",
    "    plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
    "    plt.savefig(MODEL_TYPE.upper() + '_' + PL.upper() + '_Training_Validation_Loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'test':\n",
    "    test_base = \"./CodeBLEU/test_files\"\n",
    "    if not os.path.isdir(test_base):\n",
    "        os.mkdir(test_base)\n",
    "    test_base = os.path.join(test_base, MODEL_TYPE + '_' + PL)\n",
    "    if not os.path.isdir(test_base):\n",
    "        os.mkdir(test_base)\n",
    "     \n",
    "    inp_file = open(test_base + '/input.txt', 'w')\n",
    "    hyp_file = open(test_base + '/hypothesis.txt', 'w')\n",
    "    ref_file = open(test_base + '/reference.txt', 'w')\n",
    "        \n",
    "    print(\"Testing.....\")\n",
    "    batch_num = 0\n",
    "    inputs, hypothesis, reference = [], [], []\n",
    "    for batch in test_loader:\n",
    "        # Dissect batch\n",
    "        input_ids, input_mask, target_ids, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        target_ids = target_ids.to(device)\n",
    "        # target_mask = target_mask.to(device)\n",
    "\n",
    "        output_ids = model.generate(input_ids = input_ids, attention_mask=input_mask)\n",
    "        \n",
    "        # Save results for CodeBleu Later on\n",
    "        inputs.extend(input_ids)\n",
    "        hypothesis.extend(output_ids)\n",
    "        reference.extend(target_ids)\n",
    "        \n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print(\"Batch {} done!\".format(batch_num))\n",
    "        \n",
    "    # Covert and save decoded values \n",
    "    tokenizer = tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    decoded_input = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "    decoded_hyp = tokenizer.batch_decode(hypothesis, skip_special_tokens=True)\n",
    "    decoded_refs = tokenizer.batch_decode(reference, skip_special_tokens=True)\n",
    "    \n",
    "    for i, h, r in zip(decoded_input, decoded_hyp, decoded_refs):\n",
    "        inp_file.write(i + '\\n')\n",
    "        hyp_file.write(' '.join(h.split()) + '\\n')\n",
    "        ref_file.write(r + '\\n')\n",
    "            \n",
    "inp_file.close()\n",
    "hyp_file.close()\n",
    "ref_file.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZSv5vrlc2HR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CodeT5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
