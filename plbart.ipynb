{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd317e5-473c-45ee-b476-e54589ffb266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from torchtext.data import utils\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from StaQCDataset import get_dataloader, get_datasets\n",
    "from transformers import PLBartModel, PLBartTokenizer, TrainingArguments, Trainer, PLBartForConditionalGeneration, get_scheduler\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ee48a-0edb-4923-a27d-a0819546139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5a366-d667-49d1-8976-1a6542311a98",
   "metadata": {},
   "source": [
    "## Set Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d923c4-7881-4daf-bea3-e507ce4d6d55",
   "metadata": {},
   "source": [
    "#### For Local Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98943536-bae1-412d-b3ca-9232af3e9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Constants\n",
    "BATCH_SIZE = 8\n",
    "EPOCH_SIZE = 5\n",
    "LEARNING_RATE = .0001\n",
    "NL_SEQ_LEN = 20\n",
    "CODE_SEQ_LEN = 150\n",
    "\n",
    "RESUME_EPOCH = 0                            # Epoch to resume at (0 to start from the beginning)\n",
    "\n",
    "LOG_STEP = 1                                # Frequency of epoch's for logging\n",
    "PARAMS_FILE = './training_parameters.json'  # File where input parameters (from cmdline) are stored\n",
    "PL = 'python'                               # Programming language for fine-tuning the pretrained model\n",
    "DATA_SIZE = 2000                            # Size of the raw dataset that will be used (batch size * 4 is just for testing)\n",
    "MODEL_PATH = './checkpoints/plbart_' + PL   # path to store the trained model\n",
    "MODEL_TYPE = 'plbart'\n",
    "LOG_FILE = './logs/log_' + MODEL_TYPE + '_' + PL + '.txt'   # Base file path for storing model training \n",
    "MODE = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cc97c-01b7-43d9-9c9d-2730aa3fdfaa",
   "metadata": {},
   "source": [
    "#### For rlogin training (COMMENT OUT WHEN RUNNING IPYNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055079ec-c4f8-40d4-854f-4673da8023d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Enviornment Variables from extenal file \n",
    "if os.path.exists(PARAMS_FILE):\n",
    "    # params = pd.read_csv(PARAMS_FILE)\n",
    "    with open(PARAMS_FILE) as f:\n",
    "        params = json.load(f)\n",
    "    BATCH_SIZE = params['batch_size']\n",
    "    EPOCH_SIZE = params['epoch_size']\n",
    "    LEARNING_RATE = params['learning_rate']\n",
    "    NL_SEQ_LEN = params['nl_seq_len']\n",
    "    CODE_SEQ_LEN = params['code_seq_len']\n",
    "    RESUME_EPOCH = params['resume_epoch']\n",
    "    PL = params['pl_task']\n",
    "    DATA_SIZE = params['data_size']\n",
    "    MODEL_TYPE = params['pretrained']\n",
    "    LOG_STEP = params['log_step']\n",
    "    LOG_FILE = './logs/log_' + params['pretrained'] + '_' + params['pl_task'] + '.txt'\n",
    "    MODE = params['mode']\n",
    "    \n",
    "    # Check if files exist       \n",
    "    MODEL_PATH = './checkpoints/' + params['pretrained'] + '_' + params['pl_task']\n",
    "    if not os.path.isdir(MODEL_PATH):\n",
    "        os.mkdir(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497c150-276c-4545-975c-5ee6c51b93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RUNNING: {} with pl={}, mode={}, batch_size={}, epoch_size={}, learning_rate={}, nl_seq_len={}, code_seq_len={}, data_size={}, log_step={}, log_file={}, model_dir={} resume_epoch={}\".format(MODEL_TYPE, PL, MODE, BATCH_SIZE, EPOCH_SIZE, LEARNING_RATE, NL_SEQ_LEN, CODE_SEQ_LEN, DATA_SIZE, LOG_STEP, LOG_FILE, MODEL_PATH, RESUME_EPOCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98352833-8e88-43c8-84b6-48db6088fba1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8af143-513c-49e4-80b1-d536d3702222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(data, tokenizer=None, seq_len=(20, 150)):\n",
    "    # Dissect data\n",
    "    code, nl = list(data['code']), list(data['nl'])\n",
    "    nl_len, code_len = seq_len\n",
    "    \n",
    "    # Tokenize\n",
    "    if tokenizer == None:\n",
    "        tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
    "        \n",
    "    inputs = tokenizer(nl, return_tensors='pt', max_length=nl_len, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(code, return_tensors=\"pt\", max_length=code_len, padding=\"max_length\", truncation=True)\n",
    "    # with tokenizer.as_target_tokenizer():\n",
    "    #     labels = tokenizer(code, return_tensors=\"pt\", max_length=code_len, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    InputFeatures = namedtuple(\"InputFeatures\", \"input_tokens input_ids input_mask target_tokens target_ids target_mask\")\n",
    "    \n",
    "    # Masked input Ids\n",
    "    # mask_label_ids = torch.where(labels.attention_mask == 1, labels.input_ids, torch.tensor(-100))\n",
    "    \n",
    "    data = InputFeatures(None, inputs.input_ids, inputs.attention_mask, None, labels.input_ids, labels.attention_mask)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d6026-cf7f-4fa5-a9d2-7dc2ea01b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DataLoader\n",
    "data_path = './staqc_data/' + PL \n",
    "train_loader, test_loader, val_loader = get_dataloader(data_path, tokenize_func, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       data_size=DATA_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040d6d9-371b-4c61-b2e3-fc0ac65e46be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create or Load the PLBART Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d4cfa-34b8-46b5-a901-54dbc4a79133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load trained and saved model if needed\n",
    "model = PLBartForConditionalGeneration.from_pretrained(\"uclanlp/plbart-base\")\n",
    "if RESUME_EPOCH > 0:\n",
    "    saved_model_path = '{}/Epoch{}.pkl'.format(MODEL_PATH, str(RESUME_EPOCH))\n",
    "    if os.path.exists(saved_model_path):\n",
    "        model.load_state_dict(torch.load(saved_model_path))\n",
    "    else:\n",
    "        print(\"WARNING: {} saved model does not exist! Training {} model from the epoch 0.\".format(saved_model_path, MODEL_TYPE))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf45ab-54ce-46f8-a12a-82159118f003",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5c832-ca91-4197-82b1-3fdfac5470e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'train':  \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Create a training scheduler for learning rate stepping\n",
    "    num_training_steps = EPOCH_SIZE * len(train_loader)\n",
    "    lr_scheduler = get_scheduler(name='linear', optimizer=optimizer, num_training_steps=num_training_steps, num_warmup_steps=0)\n",
    "\n",
    "    # Log training\n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "\n",
    "    # open log file\n",
    "    if RESUME_EPOCH > 0:\n",
    "        log_f = open(LOG_FILE, 'a')\n",
    "    else:\n",
    "        log_f = open(LOG_FILE, 'w')\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(RESUME_EPOCH, EPOCH_SIZE):\n",
    "\n",
    "        # Initialize batch loss vars\n",
    "        model.train()\n",
    "        running_loss = list()\n",
    "        for batch in train_loader:\n",
    "\n",
    "            # Dissect batch\n",
    "            _, input_ids, _, _, target_ids, _ = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            # input_mask = input_mask.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            # target_mask = target_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids = input_ids, labels=target_ids)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            # Update Gradients\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        curr_train_loss = np.array(running_loss).mean()\n",
    "        train_loss.append(curr_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_loss = list()\n",
    "        for batch in val_loader:\n",
    "            # Dissect batch\n",
    "            _, input_ids, _, _, target_ids, _ = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            # input_mask = input_mask.to(device)\n",
    "            target_ids = target_ids.to(device)\n",
    "            # target_mask = target_mask.to(device)\n",
    "\n",
    "            # Forward Pass through the model\n",
    "            outputs = model(input_ids = input_ids, labels=target_ids)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = outputs.loss\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "        curr_val_loss = np.array(running_loss).mean()\n",
    "        val_loss.append(curr_val_loss)\n",
    "\n",
    "        # Log loss for epoch\n",
    "        if epoch % LOG_STEP == 0:\n",
    "            loss_log = \"Epoch: [{}/{}], Training Loss: {:02.6f}, Validation Loss: {:02.6f}\".format(epoch + 1, EPOCH_SIZE, curr_train_loss, curr_val_loss)\n",
    "            log_f.write(loss_log + '\\n')\n",
    "            print(loss_log)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save(model.state_dict(), '{}/Epoch{}.pkl'.format(MODEL_PATH, str(epoch+1)))\n",
    "\n",
    "        # Quit if the the validation loss passes training loss for more than 2 epochs\n",
    "        val_pass = True\n",
    "        for t, v in zip(train_loss[-2:], val_loss[-2:]):\n",
    "            val_pass = val_pass and (v > t)\n",
    "\n",
    "        if val_pass and len(train_loss) > 3:\n",
    "            print(\"EARLY STOP: Validation loss was greater than training loss for the last 3 epochs!\")\n",
    "            break\n",
    "\n",
    "    log_f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63647b8-1168-473c-983d-c22d7ee60741",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Graph Training vs Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dacc76-aad8-43eb-a589-45fcd4b67daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'train':\n",
    "    train_loss, val_loss = list(), list()\n",
    "    for line in open(LOG_FILE):\n",
    "        split_line = line.split(' ')\n",
    "        train_loss.append(float(split_line[4].strip().replace(',', '')))  # Pull training loss from logs (from rlogin)\n",
    "        val_loss.append(float(split_line[7].strip()))    # pull validation loss from logs\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(MODEL_TYPE.upper() + \" Loss vs Epoch\")\n",
    "    plt.plot(train_loss,'k')\n",
    "    plt.plot(val_loss,'y')\n",
    "    plt.legend([\"Training Loss\",\"Validation Loss\"])\n",
    "    plt.savefig(MODEL_TYPE.upper() + '_' + PL.upper() + '_Training_Validation_Loss.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7632d78-b319-4167-acde-868aa4e5d17d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb3c71c-fd1f-44e7-88d6-25232da2f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == 'test':\n",
    "    test_base = \"./CodeBLEU/test_files\"\n",
    "    if not os.path.isdir(test_base):\n",
    "        os.mkdir(test_base)\n",
    "    test_base = os.path.join(test_base, MODEL_TYPE + '_' + PL)\n",
    "    if not os.path.isdir(test_base):\n",
    "        os.mkdir(test_base)\n",
    "     \n",
    "    inp_file = open(test_base + '/input.txt', 'w')\n",
    "    hyp_file = open(test_base + '/hypothesis.txt', 'w')\n",
    "    ref_file = open(test_base + '/reference.txt', 'w')\n",
    "        \n",
    "    print(\"Testing.....\")\n",
    "    batch_num = 0\n",
    "    inputs, hypothesis, reference = [], [], []\n",
    "    model.eval()\n",
    "    for batch in test_loader:\n",
    "        # Dissect batch\n",
    "        _, input_ids, _, _, target_ids, _ = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        \n",
    "        # Generate ids with input batch\n",
    "        output_ids = model.generate(input_ids = input_ids)\n",
    "        \n",
    "        # Save results for CodeBleu Later on\n",
    "        inputs.extend(input_ids)\n",
    "        hypothesis.extend(output_ids)\n",
    "        reference.extend(target_ids)\n",
    "        \n",
    "        batch_num += 1\n",
    "        if batch_num % 100 == 0:\n",
    "            print(\"Batch {} done!\".format(batch_num))\n",
    "        \n",
    "    # Covert and save decoded values \n",
    "    tokenizer = PLBartTokenizer.from_pretrained(\"uclanlp/plbart-base\")\n",
    "    decoded_input = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
    "    decoded_hyp = tokenizer.batch_decode(hypothesis, skip_special_tokens=True)\n",
    "    decoded_refs = tokenizer.batch_decode(reference, skip_special_tokens=True)\n",
    "    \n",
    "    for i, h, r in zip(decoded_input, decoded_hyp, decoded_refs):\n",
    "        inp_file.write(i + '\\n')\n",
    "        hyp_file.write(h + '\\n')\n",
    "        ref_file.write(r + '\\n')\n",
    "            \n",
    "inp_file.close()\n",
    "hyp_file.close()\n",
    "ref_file.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1332cfc6-e84f-465c-a25f-b16ea8bc5fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
