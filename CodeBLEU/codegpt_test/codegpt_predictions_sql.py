# -*- coding: utf-8 -*-
"""CodeGPT_predictions_sql.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DuEJsVuJME8dR4UD66UqQ3btzlIR0HDy
"""

#!pip -q install transformers sentencepiece

import pandas as pd
import numpy as np 
from transformers import GPT2Tokenizer,GPT2LMHeadModel,GPT2Config
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader,TensorDataset
import pickle
from tqdm import tqdm
from sklearn import model_selection as M
from matplotlib import pyplot as plt

if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'

device

CodeGPT = GPT2LMHeadModel.from_pretrained("microsoft/CodeGPT-small-py-adaptedGPT2")
tokenizer = GPT2Tokenizer.from_pretrained("microsoft/CodeGPT-small-py-adaptedGPT2",do_lower_case=True, bos_token='<s>', eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='concode_elem_sep')
config = GPT2Config.from_pretrained("microsoft/CodeGPT-small-py-adaptedGPT2")

CodeGPT.resize_token_embeddings(len(tokenizer))

CodeGPT.config.bos_token_id = tokenizer.bos_token_id
CodeGPT.config.eos_token_id = tokenizer.eos_token_id
CodeGPT.config.pad_token_id = tokenizer.pad_token_id
CodeGPT = CodeGPT.to(device)

test_data = pd.read_csv("./Single_Ans_sql.test.csv")

class concodeDataset(Dataset):
    def __init__(self, tokenizer, data, file_type='train', block_size=150, mode='train'):

            self.block_size = block_size
            self.mode = mode
            self.inputs = []
            self.token_labels = []

            datas = data

            length = len(datas)

            for idx in range(len(datas)):
                x = datas.iloc[idx]
                code = tokenizer.encode(x["Code"])
                nl = tokenizer.encode(x["NL"])

                input_ids, input_labels = self.pad_and_get_mask(code, nl, tokenizer)
                self.inputs.append(input_ids)
                self.token_labels.append(input_labels)


    def pad_and_get_mask(self, code, nl, tokenizer):
        if self.mode == 'test':
            code = []
        while (len(code) + len(nl) + 2 > self.block_size):
            if (len(code) > len(nl)):
                code = code[:-1]
            else:
                nl = nl[:-1]
        if self.mode == 'train':
            inputs = nl + [tokenizer.bos_token_id] + code + [tokenizer.eos_token_id]
            labels = [1] * len(nl) + [2] * (len(code)+1) + [0]
        else:
            inputs = nl + [tokenizer.bos_token_id]
            labels = [1] * len(nl) + [2]
            return inputs, labels
        assert len(inputs) <= self.block_size
        pad_len = self.block_size - len(inputs)
        inputs += [tokenizer.pad_token_id] * pad_len
        labels += [0] * pad_len
        assert len(inputs) == len(labels)
        return inputs, labels


    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, item):
        return torch.tensor(self.inputs[item]), torch.tensor(self.token_labels[item])

#dataset = concodeDataset(tokenizer, args, logger, file_type=file_type, block_size=args.block_size, mode='test')
test_dataset = concodeDataset(tokenizer, test_data,mode='test', file_type='dev',block_size=150)
#test_sampler = SequentialSampler(dataset)
test_dataloader = DataLoader(test_dataset, batch_size=1)
CodeGPT.load_state_dict(torch.load("CodeGPT-7-sql.pkl"))
CodeGPT.zero_grad()
CodeGPT.eval()
preds = []
max_gen_len = 100
step = 0
for (batch, token_labels) in tqdm(test_dataloader):
  step+=1
  if step >= 2000:
    step=0
    break
  inputs = batch.to(device)
  with torch.no_grad():
    outputs = CodeGPT.generate(inputs, max_length=150, num_beams=10, temperature=0.7, early_stopping=False, top_k=70, \
                               bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)
        #     # outputs = model.generate(inputs, max_length=args.block_size, do_sample=True, temperature=0.7, top_k=70, top_p=0.95, \
        #     #         bos_token_id=tokenizer.bos_token_id, eos_token_id=tokenizer.pad_token_id, pad_token_id=tokenizer.pad_token_id)
        #     # outputs = model.generate(inputs, max_length=args.block_size, num_beams=10, temperature=0.7, early_stopping=False, top_k=70)
        #     # outputs = model.generate(inputs, max_length=args.block_size, do_sample=True, temperature=0.7, top_k=70, top_p=0.95)
    generation = tokenizer.decode(outputs[0])[len(tokenizer.decode(inputs[0])):]
    preds.append(generation.rstrip("<pad>"))

d = pd.DataFrame({'predicted_codes':preds})
d.to_csv("SQL_predictions_CodeGPT.csv")
