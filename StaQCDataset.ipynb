{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c5b9ce-ea8a-4c70-8930-c2028b2156da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suchi_yj2wyaw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aed6c87c-dc58-4772-9735-33f93dff2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaQCDataset(data.Dataset):\n",
    "    \"\"\"Custom Dataset for pseudocode - code pairs\"\"\"\n",
    "    def __init__(self, root, tokenizer_func, tokenizer=None, max_seq_len=(20,150), data_size=None, data_cleaner=None):\n",
    "\n",
    "        if data_cleaner is None:  # If the dataset is a DeepPseudo dataset\n",
    "            # Load Data\n",
    "            data = pd.read_csv(root)\n",
    "            \n",
    "            # Preprocess and clean the dataset\n",
    "            data.rename(columns={'Code':'code', 'NL':'nl'}, inplace=True)\n",
    "            \n",
    "            if data_size != None:\n",
    "                data = data[:data_size]\n",
    "            \n",
    "            data['code'] = [line.strip() for line in data['code']]\n",
    "            data['nl'] = [line.strip().lower() for line in data['nl']]\n",
    "            \n",
    "        else:                     \n",
    "        # If the dataset is not from StacQC a custom cleaning function \n",
    "        # could be used to create similar format\n",
    "            data = data_cleaner(root)\n",
    "          \n",
    "        # Tokenize the cleaned dataset\n",
    "        self.data = data\n",
    "        \n",
    "        self.samples = tokenizer_func(data, tokenizer, max_seq_len)\n",
    "        self.input_ids = self.samples.input_ids\n",
    "        self.input_mask = self.samples.input_mask\n",
    "        self.target_ids = self.samples.target_ids\n",
    "        self.target_mask = self.samples.target_mask\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns one elemet of the dataset \"\"\"\n",
    "            \n",
    "        return self.input_ids[index], self.input_mask[index], self.target_ids[index], self.target_mask[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7fb803-c2a6-49b6-9f62-0b85c28623dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(root, tokenizer_func, data_size, train, test, val, max_seq_len, tokenizer=None, data_cleaner=None):\n",
    "    # Ensure inputs are in the ranges expected\n",
    "    #if data_size > 85000:\n",
    "    #   raise ValueError(\"data_size must be below the maximum size of dataset: 85K.\")\n",
    "    if train + test + val != 1:\n",
    "        raise ValueError(\"Sum of train, test and val fractions must add to 1!\")\n",
    "        \n",
    "    # Create Train, Test, Validation Splits when smaller data is requested for testing\n",
    "    train_size = int(train*data_size) if data_size != None else None\n",
    "    test_size = int(data_size * test) if data_size != None else None\n",
    "    val_size = (data_size - train_size - test_size) if data_size != None else None\n",
    "    \n",
    "    train_data = StaQCDataset(os.path.join(root, 'train.csv'), tokenizer_func, tokenizer, max_seq_len, train_size, data_cleaner)\n",
    "    test_data = StaQCDataset(os.path.join(root, 'test.csv'), tokenizer_func, tokenizer, max_seq_len, test_size, data_cleaner)\n",
    "    val_data = StaQCDataset(os.path.join(root, 'val.csv'), tokenizer_func, tokenizer, max_seq_len, val_size, data_cleaner)\n",
    "    \n",
    "#     if os.path.isdir(\"./staqc_data/train\") and os.path.isdir(\".staqc_data/test\"):\n",
    "#         pass\n",
    "#     else:\n",
    "#         # Initialize custom dataset\n",
    "#         dataset = StaQCDataset(root, tokenizer_func, tokenizer, max_seq_len, data_size, data_cleaner)\n",
    "\n",
    "#         data_size = len(dataset)\n",
    "\n",
    "#         # Create Train, Test, Validation Splits\n",
    "#         train_size = int(train*data_size)\n",
    "#         test_size = int(data_size * test)\n",
    "#         val_size = data_size - train_size - test_size\n",
    "#         train_data, test_data, val_data = data.random_split(dataset, [train_size, test_size, val_size])\n",
    "               \n",
    "    print(\"Train Size:{}\\nTest Size:{}\\nValidation Size:{}\".format(len(train_data), len(test_data), len(val_data))) \n",
    "    \n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f52b09c-9169-44d7-9014-5b69cca4d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(root, tokenizer_func, \n",
    "                   batch_size=32, \n",
    "                   max_seq_len=(20,150), \n",
    "                   data_size=None, \n",
    "                   train=.70, test=.15, val=.15, \n",
    "                   shuffle=True, num_workers=0, \n",
    "                   tokenizer=None, data_cleaner=None):\n",
    "    \"\"\" Creates a train, test, and val dataloader with the collate function\"\"\"\n",
    "    \n",
    "    train_data, test_data, val_data = get_datasets(root, tokenizer_func, data_size, train, test, val, max_seq_len, tokenizer, data_cleaner)\n",
    "    \n",
    "    # Create dataloaders for each dataset\n",
    "    train_loader = data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "    test_loader = data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    val_loader = data.DataLoader(dataset=val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, test_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a639be9-fa5c-4f34-b414-4f3d87ea677e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
